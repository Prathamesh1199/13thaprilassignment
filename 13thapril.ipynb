{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b532513a-6c11-4690-b75e-7c2ee1157fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "1:\n",
    "  Random Forest Regressor is a machine learning algorithm that belongs to the family of ensemble\n",
    "techniques. It is used for regression problems and is based on the idea of combining multiple\n",
    "decision trees to form a more accurate and robust model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5870c11-625b-4a6d-a13b-242f079af196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15744a5-5a46-4719-a1b5-d82c08b4576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "2:\n",
    "  Random Forest Regressor reduces the risk of overfitting by creating multiple decision trees \n",
    "using random subsets of the data and a random subset of the features for each tree. This means\n",
    "that each tree is trained on a different subset of the data, which reduces the risk of overfitting \n",
    "to the training set. Additionally, the algorithm uses a technique called bagging, which involves \n",
    "averaging the predictions of multiple trees, further reducing the risk of overfitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5cfa85-ee73-4b82-9bbc-c113262c4d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae694b0-f98e-4e3a-a04b-cd04b0e1121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "3:\n",
    "  Random Forest Regressor aggregates the predictions of multiple decision trees by taking the\n",
    "average of their individual predictions. Each decision tree in the forest independently predicts\n",
    "the target variable, and the final prediction is the average of all the individual predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbd229-3989-46d6-9787-d9f98b241795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308bde3-3712-4954-8b43-71ab5056615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "4:\n",
    "The hyperparameters of Random Forest Regressor include:\n",
    ".n_estimators: the number of decision trees in the forest.\n",
    ".max_depth: the maximum depth of each decision tree.\n",
    ".min_samples_split: the minimum number of samples required to split an internal node.\n",
    ".min_samples_leaf: the minimum number of samples required to be at a leaf node.\n",
    ".max_features: the number of features to consider when looking for the best split.\n",
    ".bootstrap: whether or not to use bootstrap samples when building decision trees.\n",
    ".random_state: the seed used by the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309177d9-f61d-4462-89fb-aa2b7e833012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65cd951-bed2-4aff-b57d-e838597ba284",
   "metadata": {},
   "outputs": [],
   "source": [
    "5:\n",
    "The main difference between Random Forest Regressor and Decision Tree Regressor is that Random \n",
    "Forest Regressor is an ensemble method that builds multiple decision trees and averages their\n",
    "predictions, while Decision Tree Regressor builds a single decision tree. Random Forest Regressor\n",
    "also uses a random subset of features for each tree, which helps to reduce overfitting and increase \n",
    "the robustness of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a626820-9720-4bde-ba42-ea3b98f18aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ad96b-a882-4fdf-8113-7594086f0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "6:\n",
    "  'Advantages of Random Forest Regressor include:\n",
    "1.High accuracy: Random Forest Regressor is known for its high accuracy, especially when compared to single decision trees.\n",
    "2.Robustness: Random Forest Regressor is less prone to overfitting than single decision trees, due to the use of multiple trees and random subsets of features.\n",
    "3.Non-parametric: Random Forest Regressor is a non-parametric model, which means it can capture complex relationships between features and target variables without making assumptions about the distribution of the data.\n",
    "4.Versatility: Random Forest Regressor can be used for both regression and classification tasks.\n",
    "\n",
    "\n",
    " 'Disadvantages of Random Forest Regressor include:\n",
    "1.Interpretability: Random Forest Regressor can be difficult to interpret, as the individual trees are not easily understandable on their own.\n",
    "2.Computationally expensive: Random Forest Regressor can be computationally expensive, especially for large datasets with many features.\n",
    "3.Training time: Random Forest Regressor can take longer to train than single decision trees.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefa2e0-a386-461c-96db-6b9ed006cf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f3416c-747d-4ceb-831a-70a8d4d125eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "7:\n",
    " The output of Random Forest Regressor is a continuous numerical value, which represents the \n",
    "predicted value of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205bcbd-ce74-444e-9db2-54eb55b5008c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d0bd0-90dd-4155-96dd-3306ca4e7def",
   "metadata": {},
   "outputs": [],
   "source": [
    "8:\n",
    "Yes, Random Forest Regressor can also be used for classification tasks by modifying the algorithm \n",
    "to predict categorical variables instead of continuous variables. In this case, the output of the\n",
    "model would be the predicted class label, rather than a numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e616e7-d21a-406d-afee-8e24a4dde6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de340eb2-7643-4aae-8734-f6cb4425df53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
